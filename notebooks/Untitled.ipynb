{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e7d0c7-6dd0-4847-9b8b-a35708f05178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/enric/venvs/dns/lib/python3.8/site-packages/df/io.py:9: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from df import enhance, init_df\n",
    "from os.path import join as pjoin\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tqdm\n",
    "import time\n",
    "from IPython.display import Audio #listen: ipd.Audio(real.detach().cpu().numpy(), rate=FS)\n",
    "import numpy as np\n",
    "import scipy.signal as sig\n",
    "import pandas as pd\n",
    "import torchmetrics.audio as M\n",
    "from speechmos import dnsmos\n",
    "from datetime import datetime\n",
    "from torchaudio.pipelines import SQUIM_OBJECTIVE, SQUIM_SUBJECTIVE\n",
    "import evaluation_metrics.calculate_intrusive_se_metrics as intru\n",
    "import evaluation_metrics.NISQA.nisqa.NISQA_lib as NL\n",
    "from evaluation_metrics.nisqa_utils import load_nisqa_model\n",
    "from evaluation_metrics.nisqa_utils import predict_nisqa\n",
    "\n",
    "import evaluation_metrics.calculate_phoneme_similarity as phon\n",
    "from evaluation_metrics.calculate_phoneme_similarity import LevenshteinPhonemeSimilarity\n",
    "\n",
    "from espnet2.bin.spk_inference import Speech2Embedding\n",
    "import evaluation_metrics.calculate_speaker_similarity as spksim\n",
    "\n",
    "from discrete_speech_metrics import SpeechBERTScore\n",
    "import evaluation_metrics.calculate_speechbert_score as sbert\n",
    "\n",
    "import evaluation_metrics.calculate_wer as wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a570c8-711b-4512-bac1-4318f1c7342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HELPER FUCTIONS\n",
    "def rep_list(short, long):\n",
    "    #repeat a list until the length of a longer one\n",
    "    reps = int(np.ceil(len(long) / len(short)))\n",
    "    short *= reps\n",
    "    short = short[:len(long)]\n",
    "    return short\n",
    "    \n",
    "def plot_tensor(x):\n",
    "    plt.plot(x.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "def extend_signal(signal, target_length):\n",
    "    \"\"\"\n",
    "    Extend a signal by repeating it if it's shorter than the target length.\n",
    "    \n",
    "    Args:\n",
    "    signal (torch.Tensor): Input signal.\n",
    "    target_length (int): Desired length of the extended signal.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Extended signal.\n",
    "    \"\"\"\n",
    "    current_length = signal.size(0)\n",
    "    if current_length < target_length:\n",
    "        repetitions = target_length // current_length\n",
    "        remainder = target_length % current_length\n",
    "        extended_signal = signal.repeat(repetitions)\n",
    "        if remainder > 0:\n",
    "            extended_signal = torch.cat((extended_signal, signal[:remainder]), dim=0)\n",
    "        return extended_signal\n",
    "    else:\n",
    "        return signal\n",
    "\n",
    "def load_audio(apath):\n",
    "    audio, fs = torchaudio.load(apath)\n",
    "    if fs != FS:\n",
    "        #print('resampling')\n",
    "        resampler = torchaudio.transforms.Resample(fs, FS)\n",
    "        audio = resampler(audio)    \n",
    "    if len(audio.shape) > 1:\n",
    "            audio = audio[0,:]\n",
    "    return audio\n",
    "\n",
    "def power(signal):\n",
    "    return np.mean(signal**2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c5e4f8-4fcb-4a14-8bc9-218cf66623e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FS = 48000\n",
    "DURATION = 4 #time in seconds of the eval chunk\n",
    "\n",
    "# names of the model folders (checkpoints/..) and aliases\n",
    "TRAINRIR_NAMES = {'D01_sb_none_NH_mono': 'singleband' , 'D02_mb_none_NH_mono': 'multiband', \n",
    "            'D03_mb_rec_NH_left': 'recdirectivity'}#, 'D05_mb_srcrec_NH_left': 'recsourcedirectivity',\n",
    "            #'D00_DNS5': 'DNS5', 'D09_SSmp3d_left' : 'soundspaces'}\n",
    "\n",
    "use_gpu = True\n",
    "if torch.cuda.is_available() and use_gpu:\n",
    "    TORCH_DEVICE = \"cuda\"\n",
    "else:\n",
    "    TORCH_DEVICE = \"cpu\"\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 8\n",
    "reverberant_noises = True\n",
    "speech_path = '/home/ubuntu/Data/DFN/textfiles/readspeech_set.txt'\n",
    "noise_path = '/home/ubuntu/Data/DFN/textfiles/test_set_noise.txt'\n",
    "dns_mos_path = '/home/ubuntu/enric/DNS-Challenge/DNSMOS/DNSMOS'\n",
    "rir_paths = ['/home/ubuntu/enric/guso_interspeech24/real_rirs.txt']\n",
    "rir_path = rir_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4c130b-6082-4b7b-a972-2b62300da7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFN_dataset(Dataset):\n",
    "    def __init__(self, speech_path, noise_path, rir_path, reverberant_noises):\n",
    "        # we store the textfile path in the class\n",
    "        print('Initializing dataset...')\n",
    "        self.speech_path = speech_path\n",
    "        self.noise_path = noise_path\n",
    "        self.rir_path = rir_path\n",
    "        self.reverberant_noises = reverberant_noises\n",
    "        \n",
    "        # load speech wav paths from the textfile\n",
    "        self.speech_paths = []\n",
    "        with open(speech_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                self.speech_paths.append(line.rstrip()) \n",
    "        print('speech set loaded. contains '+str(len(self.speech_paths)) +' files.')\n",
    "        '''\n",
    "        errors = []\n",
    "        with open('dns_test_errors.txt', 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                errors.append(line.rstrip()) \n",
    "        self.speech_paths = [item for item in self.speech_paths if item not in errors]\n",
    "        '''\n",
    "        # we filter out all speech that does not come from read_speech\n",
    "        #self.speech_paths = [item for item in self.speech_paths if item.split('/')[7]=='read_speech']\n",
    "        \n",
    "        self.snrs = np.random.uniform(low = 0, high = 30, size = len(self.speech_paths))\n",
    "        # load noise paths\n",
    "        self.noise_paths = []\n",
    "        with open(noise_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                self.noise_paths.append(line.rstrip()) \n",
    "\n",
    "        # load rir paths\n",
    "        self.rir_paths = []\n",
    "        with open(rir_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                self.rir_paths.append(line.rstrip()) \n",
    "\n",
    "        self.noise_paths = rep_list(self.noise_paths, self.speech_paths)\n",
    "        self.rir_paths = rep_list(self.rir_paths, self.speech_paths)\n",
    "        print('All paths loaded.')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.speech_paths)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # GENERATE THE CLEAN/NOISY PAIR\n",
    "        clean = load_audio(self.speech_paths[idx])\n",
    "        # handle corrupt \n",
    "        if len(clean) >= FS*DURATION:\n",
    "            speech_nrgy = torch.mean(clean[:FS*DURATION]**2) #we only use 4 first seconds for speed purpose\n",
    "        else:\n",
    "            speech_nrgy = torch.mean(clean **2)\n",
    "        if speech_nrgy == 0:\n",
    "            clean = load_audio(self.speech_paths[0])\n",
    "\n",
    "        noise = load_audio(self.noise_paths[int(idx % len(self.noise_paths))])\n",
    "\n",
    "        # handle corrupt rir\n",
    "        try:\n",
    "            rir = load_audio(self.rir_paths[int(idx % len(self.rir_paths))])\n",
    "        except:\n",
    "            rir = torch.zeros(FS)\n",
    "            rir[300] = 1.\n",
    "        # handle silent rir\n",
    "        rir_nrgy = torch.mean(rir**2)\n",
    "        if rir_nrgy == 0:\n",
    "            #print('silent rir')\n",
    "            rir = torch.zeros(FS)\n",
    "            rir[300] = 1.\n",
    "\n",
    "\n",
    "        # we extend speech and noise if too short\n",
    "        if len(clean) < FS * DURATION:\n",
    "            clean = extend_signal(clean, FS*DURATION)\n",
    "        if len(noise) < FS * DURATION:\n",
    "            noise = extend_signal(noise, FS*DURATION)\n",
    "\n",
    "        # back to numpy for easy conv\n",
    "        clean = clean.numpy()\n",
    "        noise = noise.numpy()\n",
    "        rir = rir.numpy()\n",
    "            \n",
    "        # we choose the signal chunk with more energy (to avoid silent chunks)\n",
    "        nchunks = len(clean) // (FS*DURATION)\n",
    "        chunks = np.split(clean[: FS * DURATION * nchunks], nchunks)\n",
    "        powers = np.array([power(x) for x in chunks])\n",
    "        clean = clean[np.argmax(powers) * FS * DURATION : (np.argmax(powers) + 1 ) *  FS * DURATION]\n",
    "        \n",
    "        nchunks = len(noise) // (FS*DURATION)\n",
    "        chunks = np.split(noise[: FS * DURATION * nchunks], nchunks)\n",
    "        powers = np.array([power(x) for x in chunks])\n",
    "        noise = noise[np.argmax(powers) * FS * DURATION : (np.argmax(powers) + 1 ) *  FS * DURATION]\n",
    "\n",
    "        #handle silent noise\n",
    "        noise_nrgy = power(noise)\n",
    "        if noise_nrgy == 0.:\n",
    "            #print('silent noise sample, using white noise')\n",
    "            noise = np.random.randn( FS * DURATION )\n",
    "\n",
    "        # we set the SNR\n",
    "        ini_snr = 10 * np.log10(power(clean) / power(noise))\n",
    "        noise_gain_db = ini_snr - self.snrs[idx]\n",
    "        noise *= np.power(10, noise_gain_db/20)\n",
    "\n",
    "        # we normalize to 0.9 if mixture is close to clipping\n",
    "        clips = np.max(np.abs(clean + noise))\n",
    "        if clips >= 0.9:\n",
    "            clips /= 0.9\n",
    "            noise /= clips\n",
    "            clean /= clips\n",
    "        # or to -18dBfs if smaller than that:\n",
    "        elif clips <= 10**(-18/20):\n",
    "            clips /= 10**(-18/20)\n",
    "            noise /= clips \n",
    "            clean /= clips    \n",
    "\n",
    "        # apply rir \n",
    "        revspeech = sig.fftconvolve(clean, rir, 'full')\n",
    "        # synchronize reverberant with anechoic\n",
    "        lag = np.where(np.abs(rir) >= 0.5*np.max(np.abs(rir)))[0][0]\n",
    "\n",
    "        revspeech = revspeech[lag:FS*DURATION + lag]\n",
    "\n",
    "        # enforce energy conservation\n",
    "        revspeech *= np.sqrt(power(clean) / power(revspeech)) \n",
    "\n",
    "        # apply RIR to noise too if needed\n",
    "        if self.reverberant_noises:\n",
    "            rnoise = sig.fftconvolve(noise, rir, 'full')\n",
    "            rnoise = rnoise[lag:FS*DURATION + lag]\n",
    "            rnoise *= np.sqrt(power(noise) / power(rnoise))\n",
    "            noise = rnoise\n",
    "        noisy = revspeech + noise\n",
    "        \n",
    "        # check for Nans\n",
    "        if np.any(np.isnan(noisy)):\n",
    "            print('noisy nan')\n",
    "        if np.any(np.isnan(clean)):\n",
    "            print('clean nan')\n",
    "        noisy = torch.from_numpy(noisy)\n",
    "        clean = torch.from_numpy(clean)\n",
    "        meta = [self.speech_paths[idx], self.noise_paths[int(idx % len(self.noise_paths))], self.rir_paths[int(idx % len(self.rir_paths))], self.snrs[idx].item()]\n",
    "        return noisy.float(), clean.float(), meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2b215c-786f-4171-9d3a-5e1502cab6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "speech set loaded. contains 41194 files.\n",
      "All paths loaded.\n"
     ]
    }
   ],
   "source": [
    "dataset = DFN_dataset(speech_path, noise_path, rir_path, reverberant_noises)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c3aaae9-43f2-4bd8-97d0-327578bcb744",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1c38c9-6e0e-45d1-b1f1-c6f9bf603d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 41194/41194 [05:06<00:00, 134.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm.tqdm(dataloader):\n",
    "        noisy, clean, meta = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef139c4a-36f8-4a05-a86b-896f13347032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0002,  0.0003,  0.0003,  ..., -0.0557, -0.0535, -0.0524]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b174a53-2822-407c-875a-8ba3583f53f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0011, -0.0016, -0.0014,  ..., -0.0097, -0.0094, -0.0077]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9d59366-2502-4e33-af7c-922812e2cd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/home/ubuntu/Data/DNS-Challenge/datasets_fullband/clean_fullband/vctk_wav48_silence_trimmed/p262/p262_312_mic2.wav',),\n",
       " ('/home/ubuntu/Data/DNS-Challenge/datasets_fullband/noise_fullband/cfFBEzId-iI.wav',),\n",
       " ('/home/ubuntu/Data/OpenAIR/falkland-palace-royal-tennis-court/mono/falkland_tennis_court_omni.wav',),\n",
       " tensor([13.1425], dtype=torch.float64)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a2e4a-4011-4792-b849-a215f5d7fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "errors = []\n",
    "with open('dns_test_errors.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        errors.append(line.rstrip()) \n",
    "speech_paths = [item for item in speech_paths if item not in errors]\n",
    "'''\n",
    "# we filter out all speech that does not come from read_speech\n",
    "#speech_paths = [item for item in speech_paths if item.split('/')[7]=='read_speech']\n",
    "\n",
    "snrs = np.random.uniform(low = 0, high = 30, size = len(speech_paths))\n",
    "# load noise paths\n",
    "noise_paths = []\n",
    "with open(noise_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        noise_paths.append(line.rstrip()) \n",
    "\n",
    "# load rir paths\n",
    "rir_paths = []\n",
    "with open(rir_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        rir_paths.append(line.rstrip()) \n",
    "\n",
    "noise_paths = rep_list(noise_paths, speech_paths)\n",
    "rir_paths = rep_list(rir_paths, speech_paths)\n",
    "print('All paths loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3254a63-2b5b-49bd-b757-390cc2330574",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "# GENERATE THE CLEAN/NOISY PAIR\n",
    "clean = load_audio(speech_paths[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4ab81-bc28-4c96-8069-badfad8b61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f388f-5c8f-4f73-a670-b8123607a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean) >= FS*DURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1d205-560d-45bd-b554-aed2e98e3f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(clean[:FS*DURATION]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce334c6-5cc6-40c7-a43a-c284c471788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle weird case where speech is silence\n",
    "if len(clean) >= FS*DURATION:\n",
    "    speech_nrgy = torch.mean(clean[:FS*DURATION]**2)\n",
    "else:\n",
    "    speech_nrgy = torch.mean(clean **2)\n",
    "if speech_nrgy == 0:\n",
    "    clean = load_audio(speech_paths[0])\n",
    "\n",
    "noise = load_audio(noise_paths[int(idx % len(noise_paths))])\n",
    "\n",
    "# handle corrupt rir\n",
    "try:\n",
    "    rir = load_audio(rir_paths[int(idx % len(rir_paths))])\n",
    "except:\n",
    "    rir = torch.zeros(FS)\n",
    "    rir[300] = 1.\n",
    "# handle silent rir\n",
    "rir_nrgy = torch.mean(rir**2)\n",
    "if rir_nrgy == 0:\n",
    "    #print('silent rir')\n",
    "    rir = torch.zeros(FS)\n",
    "    rir[300] = 1.\n",
    "\n",
    "\n",
    "# we extend speech and noise if too short\n",
    "if len(clean) < FS * DURATION:\n",
    "    clean = extend_signal(clean, FS*DURATION)\n",
    "if len(noise) < FS * DURATION:\n",
    "    noise = extend_signal(noise, FS*DURATION)\n",
    "\n",
    "# back to numpy for easy conv\n",
    "clean = clean.numpy()\n",
    "noise = noise.numpy()\n",
    "rir = rir.numpy()\n",
    "    \n",
    "# we choose the signal chunk with more energy (to avoid silent chunks)\n",
    "nchunks = len(clean) // (FS*DURATION)\n",
    "chunks = np.split(clean[: FS * DURATION * nchunks], nchunks)\n",
    "powers = np.array([power(x) for x in chunks])\n",
    "clean = clean[np.argmax(powers) * FS * DURATION : (np.argmax(powers) + 1 ) *  FS * DURATION]\n",
    "\n",
    "nchunks = len(noise) // (FS*DURATION)\n",
    "chunks = np.split(noise[: FS * DURATION * nchunks], nchunks)\n",
    "powers = np.array([power(x) for x in chunks])\n",
    "noise = noise[np.argmax(powers) * FS * DURATION : (np.argmax(powers) + 1 ) *  FS * DURATION]\n",
    "\n",
    "#handle silent noise\n",
    "noise_nrgy = power(noise)\n",
    "if noise_nrgy == 0.:\n",
    "    #print('silent noise sample, using white noise')\n",
    "    noise = np.random.randn( FS * DURATION )\n",
    "\n",
    "# we set the SNR\n",
    "ini_snr = 10 * np.log10(power(clean) / power(noise))\n",
    "noise_gain_db = ini_snr - snrs[idx]\n",
    "noise *= np.power(10, noise_gain_db/20)\n",
    "\n",
    "# we normalize to 0.9 if mixture is close to clipping\n",
    "clips = np.max(np.abs(clean + noise))\n",
    "if clips >= 0.9:\n",
    "    clips /= 0.9\n",
    "    noise /= clips\n",
    "    clean /= clips\n",
    "# or to -18dBfs if smaller than that:\n",
    "elif clips <= 10**(-18/20):\n",
    "    clips /= 10**(-18/20)\n",
    "    noise /= clips \n",
    "    clean /= clips    \n",
    "\n",
    "# apply rir \n",
    "revspeech = sig.fftconvolve(clean, rir, 'full')\n",
    "# synchronize reverberant with anechoic\n",
    "lag = np.where(np.abs(rir) >= 0.5*np.max(np.abs(rir)))[0][0]\n",
    "\n",
    "revspeech = revspeech[lag:FS*DURATION + lag]\n",
    "\n",
    "# enforce energy conservation\n",
    "revspeech *= np.sqrt(power(clean) / power(revspeech)) \n",
    "\n",
    "# apply RIR to noise too if needed\n",
    "if reverberant_noises:\n",
    "    rnoise = sig.fftconvolve(noise, rir, 'full')\n",
    "    rnoise = rnoise[lag:FS*DURATION + lag]\n",
    "    rnoise *= np.sqrt(power(noise) / power(rnoise))\n",
    "    noise = rnoise\n",
    "noisy = revspeech + noise\n",
    "\n",
    "# check for Nans\n",
    "if np.any(np.isnan(noisy)):\n",
    "    print('noisy nan')\n",
    "if np.any(np.isnan(clean)):\n",
    "    print('clean nan')\n",
    "noisy = torch.from_numpy(noisy)\n",
    "clean = torch.from_numpy(clean)\n",
    "meta = [speech_paths[idx], noise_paths[int(idx % len(noise_paths))], rir_paths[int(idx % len(rir_paths))], snrs[idx].item()]\n",
    "return noisy.float(), clean.float(), meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dns",
   "language": "python",
   "name": "dns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
